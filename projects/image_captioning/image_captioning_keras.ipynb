{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Final Project\n",
    "\n",
    "In this final project you will define and train an image-to-caption model, that can produce descriptions for real world images!\n",
    "\n",
    "<img src=\"images/encoder_decoder.png\" style=\"width:70%\">\n",
    "\n",
    "Model architecture: CNN encoder and RNN decoder. \n",
    "(https://research.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T10:16:46.508273Z",
     "start_time": "2017-08-27T10:16:46.506062Z"
    }
   },
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T12:30:35.584796Z",
     "start_time": "2017-09-17T12:30:35.581343Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:05.229736Z",
     "start_time": "2017-09-17T14:31:56.495874Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import utils\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Takes 10 hours and 20 GB. We've downloaded necessary files for you.\n",
    "\n",
    "Relevant links (just in case):\n",
    "- train images http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
    "- validation images http://msvocds.blob.core.windows.net/coco2014/val2014.zip\n",
    "- captions for both train and validation http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T10:23:45.863881Z",
     "start_time": "2017-08-27T10:23:45.861693Z"
    }
   },
   "source": [
    "# Extract image features\n",
    "\n",
    "We will use pre-trained InceptionV3 model for CNN encoder (https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html) and extract its last hidden layer as an embedding:\n",
    "\n",
    "<img src=\"images/inceptionv3.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:09.629321Z",
     "start_time": "2017-09-17T14:32:09.627108Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:12.621413Z",
     "start_time": "2017-09-17T14:32:11.986281Z"
    }
   },
   "outputs": [],
   "source": [
    "# load prepared embeddings\n",
    "train_img_embeds = utils.read_pickle(\"train_img_embeds.pickle\")\n",
    "train_img_fns = utils.read_pickle(\"train_img_fns.pickle\")\n",
    "val_img_embeds = utils.read_pickle(\"val_img_embeds.pickle\")\n",
    "val_img_fns = utils.read_pickle(\"val_img_fns.pickle\")\n",
    "# check shapes\n",
    "print(train_img_embeds.shape, len(train_img_fns))\n",
    "print(val_img_embeds.shape, len(val_img_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract captions for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:24.897276Z",
     "start_time": "2017-09-17T14:32:22.942805Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract captions from zip\n",
    "def get_captions_for_fns(fns, zip_fn, zip_json_path):\n",
    "    zf = zipfile.ZipFile(zip_fn)\n",
    "    j = json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
    "    id_to_fn = {img[\"id\"]: img[\"file_name\"] for img in j[\"images\"]}\n",
    "    fn_to_caps = defaultdict(list)\n",
    "    for cap in j['annotations']:\n",
    "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
    "    fn_to_caps = dict(fn_to_caps)\n",
    "    return list(map(lambda x: fn_to_caps[x], fns))\n",
    "    \n",
    "train_captions = get_captions_for_fns(train_img_fns, \"captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_train2014.json\")\n",
    "\n",
    "val_captions = get_captions_for_fns(val_img_fns, \"captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_val2014.json\")\n",
    "\n",
    "# check shape\n",
    "print(len(train_img_fns), len(train_captions))\n",
    "print(len(val_img_fns), len(val_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare captions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:40.637447Z",
     "start_time": "2017-09-17T14:43:40.633717Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview captions data\n",
    "train_captions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:40.932131Z",
     "start_time": "2017-09-17T14:43:40.891187Z"
    }
   },
   "outputs": [],
   "source": [
    "# special tokens  \n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    "PAD = \"#PAD#\"\n",
    "\n",
    "# split sentence into tokens (split into lowercased words)\n",
    "def split_sentence(sentence):\n",
    "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
    "\n",
    "def generate_vocabulary(train_captions):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
    "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
    "    \"\"\"\n",
    "    occ = dict()\n",
    "    for img_captions in train_captions:\n",
    "        for caption in img_captions:\n",
    "            words = split_sentence(caption)\n",
    "            for word in words:\n",
    "                if word not in occ:\n",
    "                    occ[word] = 0\n",
    "                occ[word] += 1\n",
    "    occ = {v: k for k, v in occ.items()}\n",
    "    vocab = [v for k, v in occ.items() if k >= 3]\n",
    "    \n",
    "    return sorted(vocab)\n",
    "    \n",
    "def caption_tokens_to_indices(captions, vocab):\n",
    "    \"\"\"\n",
    "    `captions` argument is an array of arrays:\n",
    "    [\n",
    "        [\n",
    "            \"image1 caption1\",\n",
    "            \"image1 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        [\n",
    "            \"image2 caption1\",\n",
    "            \"image2 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
    "    Add START and END tokens to start and end of each sentence respectively.\n",
    "    For the example above you should produce the following:\n",
    "    [\n",
    "        [\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for img_captions in captions:\n",
    "        arr_img = []\n",
    "        for caption in img_captions:\n",
    "            temp = [vocab[START]]\n",
    "            words = split_sentence(caption)\n",
    "            for word in words:\n",
    "                if word in vocab.keys():\n",
    "                    temp.append(vocab[word])\n",
    "                else:\n",
    "                    temp.append(vocab[UNK])\n",
    "            temp.append(vocab[END])\n",
    "            arr_img.append(temp)\n",
    "        res.append(arr_img)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:44.824532Z",
     "start_time": "2017-09-17T14:43:41.264769Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare vocabulary\n",
    "if \"vocab.pickle\" not in os.listdir():\n",
    "    vocab = generate_vocabulary(train_captions)\n",
    "    vocab = [PAD, START, END, UNK] + vocab\n",
    "    vocab = {token: index for index, token in enumerate(vocab)}\n",
    "    utils.save_pickle(vocab, \"vocab.pickle\")\n",
    "else:\n",
    "    vocab = utils.read_pickle(\"vocab.pickle\")\n",
    "    \n",
    "if \"vocab_inverse.pickle\" not in os.listdir():\n",
    "    vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
    "    utils.save_pickle(vocab_inverse, \"vocab_inverse.pickle\")\n",
    "else:\n",
    "    vocab_inverse = utils.read_pickle(\"vocab_inverse.pickle\")\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:53.206639Z",
     "start_time": "2017-09-17T14:43:44.826028Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace tokens with indices\n",
    "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
    "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_captions_indexed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captions have different length, but we need to batch them, that's why we will add PAD tokens so that all sentences have an equal length. \n",
    "\n",
    "We will crunch LSTM through all the tokens, but we will ignore padding tokens during loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T16:11:52.425546Z",
     "start_time": "2017-09-17T16:11:52.414004Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will use this during training\n",
    "def batch_captions_to_matrix(batch_captions, pad_idx, max_len):\n",
    "    \"\"\"\n",
    "    `batch_captions` is an array of arrays:\n",
    "    [\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        ...\n",
    "    ]\n",
    "    Add padding with pad_idx where necessary.\n",
    "    Input example: [[1, 2, 3], [4, 5]]\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
    "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
    "    Try to use numpy, we need this function to be fast!\n",
    "    \"\"\"\n",
    "    matrix = np.ones(shape=(len(batch_captions), max_len))*pad_idx\n",
    "    for i, arr in enumerate(batch_captions):\n",
    "        l = min(max_len, len(batch_captions[i]))\n",
    "        matrix[i, :l] = batch_captions[i][:l]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(val_img_embeds.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "num_cut_val_to_train = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:59.397913Z",
     "start_time": "2017-09-17T14:43:58.913391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_captions_indexed = np.array(train_captions_indexed)\n",
    "val_captions_indexed = np.array(val_captions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_embeds = np.concatenate((train_img_embeds, val_img_embeds[:20000]), axis=0)\n",
    "train_captions_indexed = np.concatenate((train_captions_indexed, val_captions_indexed[:20000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_embeds = val_img_embeds[20000:]\n",
    "val_captions_indexed = val_captions_indexed[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_captions_indexed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T13:34:09.664927Z",
     "start_time": "2017-08-27T13:34:09.662597Z"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our problem is to generate image captions, RNN text generator should be conditioned on image. The idea is to use image features as an initial state for RNN instead of zeros. \n",
    "\n",
    "Remember that you should transform image feature vector to RNN hidden state size by fully-connected layer and then pass it to RNN.\n",
    "\n",
    "During training we will feed ground truth tokens into the lstm to get predictions of next tokens. \n",
    "\n",
    "Notice that we don't need to feed last token (END) as input (http://cs.stanford.edu/people/karpathy/):\n",
    "\n",
    "<img src=\"images/encoder_decoder_explained.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T16:33:04.453351Z",
     "start_time": "2017-09-17T16:33:04.449675Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_EMBED_SIZE = 2048\n",
    "IMG_EMBED_BOTTLENECK = 128\n",
    "WORD_EMBED_SIZE = 100\n",
    "LSTM_UNITS = 256\n",
    "LOGIT_BOTTLENECK = 120\n",
    "pad_idx = vocab[PAD]\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define decoder graph.\n",
    "\n",
    "We use Keras layers where possible because we can use them in functional style with weights reuse like this:\n",
    "```python\n",
    "dense_layer = L.Dense(42, input_shape=(None, 100) activation='relu')\n",
    "a = tf.placeholder('float32', [None, 100])\n",
    "b = tf.placeholder('float32', [None, 100])\n",
    "dense_layer(a)  # that's how we applied dense layer!\n",
    "dense_layer(b)  # and again\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "M = keras.models\n",
    "L = keras.layers\n",
    "K = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tensors(nodes) in the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_embed_input = L.Input(shape=(train_img_embeds.shape[1],))\n",
    "\n",
    "img_embed_tensor = L.Dense(units=IMG_EMBED_BOTTLENECK, activation=\"elu\")\n",
    "initial_state_tensor = L.Dense(units=LSTM_UNITS, activation=\"elu\")\n",
    "\n",
    "X_decoder = L.Input(shape=(None,))\n",
    "embedding_tensor = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
    "LSTM_tensor = L.LSTM(units=LSTM_UNITS, activation=\"tanh\", return_sequences=True)\n",
    "logit_bottlekneck_tensor = L.TimeDistributed(L.Dense(LOGIT_BOTTLENECK, activation=\"elu\"))\n",
    "output_tensor = L.TimeDistributed(L.Dense(len(vocab), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect those tensors(nodes) together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = img_embed_tensor(img_embed_input)\n",
    "h0 = c0 = initial_state_tensor(flow)\n",
    "\n",
    "flow = embedding_tensor(X_decoder)\n",
    "flow = LSTM_tensor(flow, initial_state=[h0, c0])\n",
    "flow = logit_bottlekneck_tensor(flow)\n",
    "output = output_tensor(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M.Model(inputs=[img_embed_input, X_decoder], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, images, captions, batch_size, max_len, shuffle=True):\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.max_len = max_len\n",
    "        self.indices = np.arange(self.images.shape[0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        m = self.images.shape[0]\n",
    "        num_batches = m//self.batch_size\n",
    "        return (num_batches if m/self.batch_size == 0 else num_batches + 1) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        image_embed_gen = self.images[index:index+self.batch_size]\n",
    "        batch_captions = [caption[np.random.randint(5)] for caption in self.captions[index:index+self.batch_size]]\n",
    "        caption_gen = batch_captions_to_matrix(batch_captions, pad_idx, max_len)\n",
    "        return [image_embed_gen, caption_gen[:, 1:]], keras.utils.to_categorical(caption_gen[:, :-1], num_classes=len(vocab))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:00.437338Z",
     "start_time": "2017-09-17T14:44:00.434472Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(train_img_embeds, train_captions_indexed, batch_size, max_len)\n",
    "val_gen = DataGenerator(val_img_embeds, val_captions_indexed, batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"model/image_captioning.hdf5\"\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(file_path, save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "model.fit_generator(generator=train_gen, epochs=n_epochs, validation_data=val_gen, \n",
    "                    callbacks=[checkpoints, TQDMNotebookCallback()], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M.load_model(\"model/image_captioning.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take the last hidden layer of IncetionV3 as an image embedding\n",
    "preprocess_for_model = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "encoder_model = keras.applications.InceptionV3(include_top=False)\n",
    "encoder_model = M.Model(encoder_model.inputs, L.GlobalAveragePooling2D()(encoder_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:22.575410Z",
     "start_time": "2017-09-17T14:44:22.547785Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is an actual prediction loop\n",
    "def generate_caption(image, encoder_model, model, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate caption for given image.\n",
    "    \"\"\"\n",
    "    # current caption\n",
    "    # start with only START token\n",
    "    caption = [vocab[START]]\n",
    "    img_embed = encoder_model.predict(image)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        next_word_probs = model.predict([img_embed, np.array([caption])])\n",
    "        next_word_probs = next_word_probs[:, i, : ]\n",
    "        next_word = np.argmax(next_word_probs, axis=-1)\n",
    "        caption.append(next_word[0])\n",
    "        if next_word[0] == vocab[END]:\n",
    "            break\n",
    "\n",
    "    words = ' '.join(list(map(vocab_inverse.get, caption))[1:-1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_to_image_raw_bytes(raw):\n",
    "    img = utils.decode_image_from_buf(raw)\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.grid('off')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), preprocess_for_model)\n",
    "    img = img.reshape((1, IMG_SIZE, IMG_SIZE, 3))\n",
    "    print(generate_caption(img, encoder_model, model, max_len)[1:-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/her.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/me.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/her1.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/room.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/wolf.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/dog.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/he_she.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/troll.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/mock.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_image_raw_bytes(open(\"./images/mia.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T12:34:10.689158Z",
     "start_time": "2017-09-24T12:34:10.675938Z"
    }
   },
   "source": [
    "Now it's time to find 10 examples where your model works good and 10 examples where it fails! \n",
    "\n",
    "You can use images from validation set as follows:\n",
    "```python\n",
    "show_valid_example(val_img_fns, example_idx=...)\n",
    "```\n",
    "\n",
    "You can use images from the Internet as follows:\n",
    "```python\n",
    "! wget ...\n",
    "apply_model_to_image_raw_bytes(open(\"...\", \"rb\").read())\n",
    "```\n",
    "\n",
    "If you use these functions, the output will be embedded into your notebook and will be visible during peer review!\n",
    "\n",
    "When you're done, download your noteboook using \"File\" -> \"Download as\" -> \"Notebook\" and prepare that file for peer review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR EXAMPLES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-24T12:34:25.055581Z",
     "start_time": "2017-09-24T12:34:25.052373Z"
    }
   },
   "source": [
    "That's it! \n",
    "\n",
    "Congratulations, you've trained your image captioning model and now can produce captions for any picture from the  Internet!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "157px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
