{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer: Attention is all you need paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_ff = d_ff\n",
    "        self.word_embed = L.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def _format(self, block, head):\n",
    "        return str(block) + str(head)\n",
    "    \n",
    "    def _init_structure(self, decoder_part=False):\n",
    "        assert not hasattr(self, \"pos_enc\"), \"The structure is initialized already.\"\n",
    "        self.pos_enc = tf.zeros(shape=(1, self.seq_len, self.d_model))\n",
    "        for pos in range(self.seq_len):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                self.pos_enc[:, pos, i] = tf.sin(pos / (10000 ** ((2 * i)/self.d_model)))\n",
    "                self.pos_enc[:, pos, i + 1] = tf.cos(pos / (10000 ** ((2 * (i + 1))/self.d_model)))\n",
    "\n",
    "        for block_id in range(num_blocks):\n",
    "            for head_id in range(num_heads):\n",
    "                setattr(self, \"Q\" + self._format(block_id, head_id), L.Dense(self.d_k))\n",
    "                setattr(self, \"K\" + self._format(block_id, head_id), L.Dense(self.d_k))\n",
    "                setattr(self, \"V\" + self._format(block_id, head_id), L.Dense(self.d_v))\n",
    "                if decoder_part:\n",
    "                    setattr(self, \"Qenc\" + self._format(block_id, head_id), L.Dense(self.d_k))\n",
    "                    setattr(self, \"Kenc\" + self._format(block_id, head_id), L.Dense(self.d_k))\n",
    "                    setattr(self, \"Venc\" + self._format(block_id, head_id), L.Dense(self.d_v))\n",
    "            setattr(self, \"O\" + str(block_id), L.Dense(self.d_model))\n",
    "            setattr(self, \"FFN1\" + str(block_id), L.Dense(self.d_ff, activation=\"relu\"))\n",
    "            setattr(self, \"FFN2\" + str(block_id), L.Dense(self.d_model))\n",
    "            \n",
    "    def _ffn(self, block_id, attention_output):\n",
    "        ffn1 = getattr(self, \"FFN1\" + str(block_id))(attention_output)\n",
    "        ffn2 = getattr(self, \"FFN2\" + str(block_id))(ffn1)\n",
    "        return ffn2\n",
    "    \n",
    "    def _scaled_dot_product(self, Q, K, V):\n",
    "        score = tf.nn.softmax(tf.matmul(Q, K, transpose_b=True)/tf.sqrt(self.d_k), axis=-1)\n",
    "        score = tf.matmul(scores, V)\n",
    "        return score\n",
    "                \n",
    "    def _multi_head_attention(self, block_id, Q, K, V, connection_head=False):\n",
    "        head_output = []\n",
    "        for head_id in self.num_heads:\n",
    "            if connection_head:\n",
    "                Q = getattr(self, \"Qenc\" + self._format(block_id, head_id))(Q)\n",
    "                K = getattr(self, \"Kenc\" + self._format(block_id, head_id))(K)\n",
    "                V = getattr(self, \"Venc\" + self._format(block_id, head_id))(V)\n",
    "            else:\n",
    "                Q = getattr(self, \"Q\" + self._format(block_id, head_id))(Q)\n",
    "                K = getattr(self, \"K\" + self._format(block_id, head_id))(K)\n",
    "                V = getattr(self, \"V\" + self._format(block_id, head_id))(V)\n",
    "            score = self._scaled_dot_product(Q, K, V)\n",
    "            head_output.append(score)\n",
    "        head_output = tf.concat(head_output, axis=-1)\n",
    "        head_output = getattr(self, \"O\" + str(block_id))(head_output)\n",
    "        return head_output\n",
    "    \n",
    "    def _block_computation(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Transformer is abstract class. You must implement this function!\")\n",
    "        \n",
    "    def call(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Transformer is abstract class. You must implement this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Transformer):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff):\n",
    "        super(Encoder, self).__init__(num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff)\n",
    "        self._init_structure()\n",
    "    \n",
    "    def _block_computation(self, block_id, x):\n",
    "        attention_output = self._multi_head_attention(block_id, x, x, x, False)\n",
    "        attention_output = L.LayerNormalization()(attention_output + x)\n",
    "        \n",
    "        block_output = self._ffn(block_id, attention_output)\n",
    "        block_output = L.LayerNormalization()(block_output + attention_output)\n",
    "        return block_output\n",
    "    \n",
    "    def call(self, x):\n",
    "        word_embed = self.word_embed(x)\n",
    "        word_embed = word_embed + self.pos_enc\n",
    "        \n",
    "        block_output = word_embed\n",
    "        for block_id in range(self.num_blocks):\n",
    "            block_output = self._block_computation(block_id, block_output)\n",
    "        return block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Transformer):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff):\n",
    "        super(Decoder, self).__init__(num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff)\n",
    "        self._init_structure(decoder_part=True)\n",
    "        self.logits = L.Dense(units=vocab_size)\n",
    "    \n",
    "    def _block_computation(self, block_id, x, encoder_output):\n",
    "        attention_output = self._multi_head_attention(block_id, x, x, x, connection_head=False)\n",
    "        attention_output = L.LayerNormalization()(attention_output + x)\n",
    "        \n",
    "        connection_output = self._multi_head_attention(block_id, encoder_output, encoder_output, \n",
    "                                                       attention_output, connection_head=True)\n",
    "        connection_output = L.LayerNormalization()(connection_output + attention_output)\n",
    "        \n",
    "        block_output = self._ffn(block_id, connection_output)\n",
    "        block_output = L.LayerNormalization()(block_output + connection_output)\n",
    "        return block_output\n",
    "    \n",
    "    def call(self, x, encoder_output):\n",
    "        word_embed = self.word_embed(x)\n",
    "        word_embed = word_embed + self.pos_enc\n",
    "        \n",
    "        block_output = word_embed\n",
    "        for block_id in range(self.num_blocks):\n",
    "            block_output = self._block_computation(block_id, block_output, encoder_output)\n",
    "        logits = self.logits(block_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(labels, logits):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
